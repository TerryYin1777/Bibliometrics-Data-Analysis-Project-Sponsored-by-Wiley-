{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize,ne_chunk,pos_tag\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Files That Are Not Successfully Extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting on file:  savedrecs (85).txt\n",
      "This file is abnormal-> savedrecs (85).txt \n",
      "Number of records: 18\n",
      "extracting on file:  savedrecs (1).txt\n",
      "extracting on file:  savedrecs (50).txt\n",
      "extracting on file:  savedrecs (46).txt\n",
      "extracting on file:  savedrecs (11).txt\n",
      "extracting on file:  savedrecs (66).txt\n",
      "extracting on file:  savedrecs (31).txt\n",
      "extracting on file:  savedrecs (27).txt\n",
      "extracting on file:  savedrecs (70).txt\n",
      "extracting on file:  savedrecs (71).txt\n",
      "extracting on file:  savedrecs (26).txt\n",
      "extracting on file:  savedrecs (30).txt\n",
      "extracting on file:  savedrecs (67).txt\n",
      "extracting on file:  savedrecs (10).txt\n",
      "extracting on file:  savedrecs (47).txt\n",
      "extracting on file:  savedrecs (51).txt\n",
      "extracting on file:  savedrecs (84).txt\n",
      "extracting on file:  savedrecs (60).txt\n",
      "extracting on file:  savedrecs (37).txt\n",
      "extracting on file:  savedrecs (21).txt\n",
      "extracting on file:  savedrecs (76).txt\n",
      "extracting on file:  savedrecs (83).txt\n",
      "extracting on file:  savedrecs (7).txt\n",
      "extracting on file:  savedrecs (56).txt\n",
      "extracting on file:  savedrecs (40).txt\n",
      "extracting on file:  savedrecs (17).txt\n",
      "extracting on file:  savedrecs (16).txt\n",
      "extracting on file:  savedrecs (41).txt\n",
      "extracting on file:  savedrecs (57).txt\n",
      "extracting on file:  savedrecs (6).txt\n",
      "extracting on file:  savedrecs (82).txt\n",
      "extracting on file:  savedrecs (77).txt\n",
      "extracting on file:  savedrecs (20).txt\n",
      "extracting on file:  savedrecs (36).txt\n",
      "extracting on file:  savedrecs (61).txt\n",
      "extracting on file:  savedrecs (19).txt\n",
      "extracting on file:  savedrecs (58).txt\n",
      "extracting on file:  savedrecs (9).txt\n",
      "extracting on file:  savedrecs (74).txt\n",
      "extracting on file:  savedrecs (23).txt\n",
      "extracting on file:  savedrecs (35).txt\n",
      "extracting on file:  savedrecs (62).txt\n",
      "extracting on file:  savedrecs (15).txt\n",
      "This file is abnormal-> savedrecs (15).txt \n",
      "Number of records: 499\n",
      "extracting on file:  savedrecs (42).txt\n",
      "extracting on file:  savedrecs (54).txt\n",
      "extracting on file:  savedrecs (5).txt\n",
      "extracting on file:  savedrecs (78).txt\n",
      "extracting on file:  savedrecs.txt\n",
      "extracting on file:  savedrecs (39).txt\n",
      "extracting on file:  savedrecs (81).txt\n",
      "extracting on file:  savedrecs (80).txt\n",
      "extracting on file:  savedrecs (38).txt\n",
      "extracting on file:  savedrecs (79).txt\n",
      "extracting on file:  savedrecs (4).txt\n",
      "extracting on file:  savedrecs (55).txt\n",
      "extracting on file:  savedrecs (43).txt\n",
      "extracting on file:  savedrecs (14).txt\n",
      "extracting on file:  savedrecs (63).txt\n",
      "extracting on file:  savedrecs (34).txt\n",
      "extracting on file:  savedrecs (22).txt\n",
      "extracting on file:  savedrecs (75).txt\n",
      "extracting on file:  savedrecs (8).txt\n",
      "extracting on file:  savedrecs (59).txt\n",
      "extracting on file:  savedrecs (18).txt\n",
      "extracting on file:  savedrecs (13).txt\n",
      "extracting on file:  savedrecs (44).txt\n",
      "extracting on file:  savedrecs (52).txt\n",
      "extracting on file:  savedrecs (3).txt\n",
      "extracting on file:  savedrecs (29).txt\n",
      "extracting on file:  savedrecs (68).txt\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 69 fields in line 6, saw 75\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-195-665cd7e82438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extracting on file: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcurrentfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrentfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This file is abnormal->'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\nNumber of records:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrentfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 69 fields in line 6, saw 75\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for filename in filelist:\n",
    "    if filename.endswith('txt'):\n",
    "\n",
    "        print('extracting on file: ',filename)\n",
    "        filepath = os.path.join(inputpath,filename)\n",
    "        currentfile = pd.read_table(filepath,sep = '\\t',index_col=False,encoding = 'utf-8')\n",
    "        if currentfile.shape[0]<500:\n",
    "            print('This file is abnormal->',filename,'\\nNumber of records:',currentfile.shape[0])\n",
    "            n+=1\n",
    "print('There are altogather {} abnormal files.'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentfile = pd.read_csv('savedrecs (68).csv')#,sep = '\\t',index_col=False,header = None,error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(inputpath,'savedrecs (69).txt')\n",
    "currentfile = pd.read_table(filepath,sep = '\\t',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentfile['PY'] = currentfile['PY'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentfile['TC'] = currentfile['TC'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep = ['AU','AF','TI','SO','OI','SN','PD','PY','VL','BP','EP','DI','UT','TC']\n",
    "whole_column = set(currentfile.columns)\n",
    "na_column = list(whole_column-set(keep))\n",
    "currentfile.loc[:,na_column] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "currentfile.to_csv(os.path.join(outputpath,filelist[0][:-4]+'_extract_cite_from_full'+filelist[0][-4:]),index = None, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Records Number Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_by_year_records():\n",
    "    \n",
    "    all_data_store_path = '/Users/tylio/Google Drive/2018 spring/686/Wiley Sponsered Project/Data'\n",
    "    current_data_folder_name = '(40000+)_by_year'\n",
    "    current_data_store_path = os.path.join(all_data_store_path,current_data_folder_name)\n",
    "    current_data_list = [f for f in os.listdir(current_data_store_path) if f.endswith('txt')]\n",
    "    \n",
    "    for data in current_data_list:\n",
    "        \n",
    "        print('Checking on: {}'.format(data))\n",
    "        \n",
    "        from_to = re.findall(r'-(\\d+)',data)\n",
    "        num_records = int(from_to[1])-int(from_to[0])+1\n",
    "        try:\n",
    "            current_df = pd.read_table(os.path.join(current_data_store_path,data),sep = '\\t',index_col=False)\n",
    "            if current_df.shape[0]!=num_records:\n",
    "            \n",
    "                print('Actual number of records is not in accordance with expected.\\n')\n",
    "                print('Expected: {}---Actual: {}'.format(num_records,current_df.shape[0]))\n",
    "                \n",
    "        except:\n",
    "            print('Reading ERROR on {}'.format(data))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking on: 2001-1-500.txt\n",
      "Checking on: 2008-1001-1500.txt\n",
      "Checking on: 2015-1-500.txt\n",
      "Checking on: 2009-1501-2000.txt\n",
      "Checking on: 2009-2001-2102.txt\n",
      "Checking on: 1974-1-12.txt\n",
      "Checking on: 2005-1001-1206.txt\n",
      "Checking on: 1991-1-166.txt\n",
      "Checking on: 2018-1-500.txt\n",
      "Checking on: 2007-1-500.txt\n",
      "Checking on: 2013-1-500.txt\n",
      "Checking on: 1975-1-16.txt\n",
      "Checking on: 2000-501-547.txt\n",
      "Checking on: 1983-1-33.txt\n",
      "Checking on: 1998-1-456.txt\n",
      "Checking on: 1992-1-183.txt\n",
      "Checking on: 1982-1-24.txt\n",
      "Checking on: 2006-501-1000.txt\n",
      "Checking on: 2016-1001-1500.txt\n",
      "Checking on: 2014-501-1000.txt\n",
      "Checking on: 2012-1001-1500.txt\n",
      "Checking on: 2014-1-500.txt\n",
      "Checking on: 2013-501-1000.txt\n",
      "Checking on: 2002-501-729.txt\n",
      "Checking on: 2000-1-500.txt\n",
      "Checking on: 2014-2001-2500.txt\n",
      "Checking on: 2016-2501-3000.txt\n",
      "Checking on: 2013-1501-2000.txt\n",
      "Checking on: 1973-1-7.txt\n",
      "Checking on: 1978-1-17.txt\n",
      "Checking on: 1972-1-14.txt\n",
      "Checking on: 1977-1-22.txt\n",
      "Checking on: 1994-1-282.txt\n",
      "Checking on: 1993-1-224.txt\n",
      "Checking on: 2017-1501-2000.txt\n",
      "Checking on: 2017-3001-3500.txt\n",
      "Checking on: 2012-1-500.txt\n",
      "Checking on: 2012-501-1000.txt\n",
      "Checking on: 2010-1501-2000.txt\n",
      "Checking on: 2015-2501-3000.txt\n",
      "Checking on: 2015-501-1000.txt\n",
      "Checking on: 2006-1-500.txt\n",
      "Checking on: 2001-501-600.txt\n",
      "Checking on: 2014-1501-2000.txt\n",
      "Checking on: 2007-501-1000.txt\n",
      "Checking on: 2010-2001-2467.txt\n",
      "Checking on: 2014-3001-3500.txt\n",
      "Checking on: 2015-1001-1500.txt\n",
      "Checking on: 2013-2001-2500.txt\n",
      "Checking on: 2011-1001-1500.txt\n",
      "Checking on: 2017-2001-2500.txt\n",
      "Checking on: 2003-1-500.txt\n",
      "Checking on: 2006-1001-1337.txt\n",
      "Checking on: 1999-1-499.txt\n",
      "Checking on: 2015-3001-3500.txt\n",
      "Checking on: 2017-501-1000.txt\n",
      "Checking on: 1989-1-49.txt\n",
      "Checking on: 2015-1501-2000.txt\n",
      "Checking on: 2010-501-1000.txt\n",
      "Checking on: 2017-1-500.txt\n",
      "Checking on: 2012-2501-2894.txt\n",
      "Checking on: 2014-2501-3000.txt\n",
      "Checking on: 2005-501-1000.txt\n",
      "Checking on: 1981-1-24.txt\n",
      "Checking on: 2011-1501-2000.txt\n",
      "Checking on: 1985-1-38.txt\n",
      "Checking on: 2016-2001-2500.txt\n",
      "Checking on: 2010-1001-1500.txt\n",
      "Checking on: 1996-1-357.txt\n",
      "Checking on: 2007-1001-1500.txt\n",
      "Checking on: 1990-1-93.txt\n",
      "Checking on: 1976-1-16.txt\n",
      "Checking on: 2012-2001-2500.txt\n",
      "Checking on: 1984-1-39.txt\n",
      "Checking on: 2014-1001-1500.txt\n",
      "Checking on: 2015-2001-2500.txt\n",
      "Checking on: 2004-501-1000.txt\n",
      "Checking on: 2013-1001-1500.txt\n",
      "Checking on: 2011-2001-2500.txt\n",
      "Checking on: 2005-1-500.txt\n",
      "Checking on: 2017-1001-1500.txt\n",
      "Checking on: 2011-1-500.txt\n",
      "Checking on: 2011-501-1000.txt\n",
      "Checking on: 2016-501-1000.txt\n",
      "Checking on: 2007-1501-1701.txt\n",
      "Checking on: 2016-3001-3500.txt\n",
      "Checking on: 1979-1-24.txt\n",
      "Checking on: 2013-2501-3000.txt\n",
      "Checking on: 1988-1-58.txt\n",
      "Checking on: 2016-1501-2000.txt\n",
      "Checking on: 2008-1-500.txt\n",
      "Checking on: 2016-3501-3935.txt\n",
      "Checking on: 2012-1501-2000.txt\n",
      "Checking on: 1980-1-21.txt\n",
      "Checking on: 2017-2501-3000.txt\n",
      "Checking on: 2016-1-500.txt\n",
      "Checking on: 2017-3501-4000.txt\n",
      "Checking on: 2002-1-500.txt\n",
      "Checking on: 2018-501-522.txt\n",
      "Checking on: 1986-1-36.txt\n",
      "Checking on: 1995-1-300.txt\n",
      "Checking on: 2017-4001-4142.txt\n",
      "Checking on: 1997-1-381.txt\n",
      "Checking on: 2009-501-1000.txt\n",
      "Checking on: 2009-1001-1500.txt\n",
      "Checking on: 2010-1-500.txt\n",
      "Checking on: 2003-501-844.txt\n",
      "Checking on: 2004-1-500.txt\n",
      "Checking on: 1987-1-30.txt\n",
      "Checking on: 2014-3501-3633.txt\n",
      "Checking on: 2008-501-1000.txt\n",
      "Checking on: 2004-1001-1004.txt\n",
      "Checking on: 2013-3001-3222.txt\n",
      "Checking on: 2015-3501-3748.txt\n",
      "Checking on: 2011-2501-2571.txt\n",
      "Checking on: 2009-1-500.txt\n",
      "Checking on: 2008-1501-1904.txt\n"
     ]
    }
   ],
   "source": [
    "check_by_year_records()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatnate and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concat_and_save():\n",
    "    \n",
    "    all_data_store_path = '/Users/tylio/Google Drive/2018 spring/686/Wiley Sponsered Project/Data'\n",
    "    current_data_folder_name = '(40000+)_by_year'\n",
    "    current_data_store_path = os.path.join(all_data_store_path,current_data_folder_name)\n",
    "    current_data_list = [f for f in os.listdir(current_data_store_path) if f.endswith('txt')]\n",
    "    whole_file = pd.DataFrame()\n",
    "    \n",
    "    for data in current_data_list:\n",
    "        \n",
    "        print('Loading on: {}'.format(data))\n",
    "        \n",
    "        from_to = re.findall(r'-(\\d+)',data)\n",
    "        num_records = int(from_to[1])-int(from_to[0])+1\n",
    "        \n",
    "        try:\n",
    "            current_df = pd.read_table(os.path.join(current_data_store_path,data),sep = '\\t',index_col=False)              \n",
    "        except:\n",
    "            print('Reading ERROR on {}'.format(data))\n",
    "            \n",
    "        assert current_df.shape[0]==num_records\n",
    "        assert current_df.shape[1]==68\n",
    "        \n",
    "        whole_file = pd.concat([whole_file,current_df])\n",
    "    whole_file.to_csv(os.path.join(current_data_store_path,'whole_file.csv'),index = False)\n",
    "        \n",
    "    return whole_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading on: 2001-1-500.txt\n",
      "Loading on: 2008-1001-1500.txt\n",
      "Loading on: 2015-1-500.txt\n",
      "Loading on: 2009-1501-2000.txt\n",
      "Loading on: 2009-2001-2102.txt\n",
      "Loading on: 1974-1-12.txt\n",
      "Loading on: 2005-1001-1206.txt\n",
      "Loading on: 1991-1-166.txt\n",
      "Loading on: 2018-1-500.txt\n",
      "Loading on: 2007-1-500.txt\n",
      "Loading on: 2013-1-500.txt\n",
      "Loading on: 1975-1-16.txt\n",
      "Loading on: 2000-501-547.txt\n",
      "Loading on: 1983-1-33.txt\n",
      "Loading on: 1998-1-456.txt\n",
      "Loading on: 1992-1-183.txt\n",
      "Loading on: 1982-1-24.txt\n",
      "Loading on: 2006-501-1000.txt\n",
      "Loading on: 2016-1001-1500.txt\n",
      "Loading on: 2014-501-1000.txt\n",
      "Loading on: 2012-1001-1500.txt\n",
      "Loading on: 2014-1-500.txt\n",
      "Loading on: 2013-501-1000.txt\n",
      "Loading on: 2002-501-729.txt\n",
      "Loading on: 2000-1-500.txt\n",
      "Loading on: 2014-2001-2500.txt\n",
      "Loading on: 2016-2501-3000.txt\n",
      "Loading on: 2013-1501-2000.txt\n",
      "Loading on: 1973-1-7.txt\n",
      "Loading on: 1978-1-17.txt\n",
      "Loading on: 1972-1-14.txt\n",
      "Loading on: 1977-1-22.txt\n",
      "Loading on: 1994-1-282.txt\n",
      "Loading on: 1993-1-224.txt\n",
      "Loading on: 2017-1501-2000.txt\n",
      "Loading on: 2017-3001-3500.txt\n",
      "Loading on: 2012-1-500.txt\n",
      "Loading on: 2012-501-1000.txt\n",
      "Loading on: 2010-1501-2000.txt\n",
      "Loading on: 2015-2501-3000.txt\n",
      "Loading on: 2015-501-1000.txt\n",
      "Loading on: 2006-1-500.txt\n",
      "Loading on: 2001-501-600.txt\n",
      "Loading on: 2014-1501-2000.txt\n",
      "Loading on: 2007-501-1000.txt\n",
      "Loading on: 2010-2001-2467.txt\n",
      "Loading on: 2014-3001-3500.txt\n",
      "Loading on: 2015-1001-1500.txt\n",
      "Loading on: 2013-2001-2500.txt\n",
      "Loading on: 2011-1001-1500.txt\n",
      "Loading on: 2017-2001-2500.txt\n",
      "Loading on: 2003-1-500.txt\n",
      "Loading on: 2006-1001-1337.txt\n",
      "Loading on: 1999-1-499.txt\n",
      "Loading on: 2015-3001-3500.txt\n",
      "Loading on: 2017-501-1000.txt\n",
      "Loading on: 1989-1-49.txt\n",
      "Loading on: 2015-1501-2000.txt\n",
      "Loading on: 2010-501-1000.txt\n",
      "Loading on: 2017-1-500.txt\n",
      "Loading on: 2012-2501-2894.txt\n",
      "Loading on: 2014-2501-3000.txt\n",
      "Loading on: 2005-501-1000.txt\n",
      "Loading on: 1981-1-24.txt\n",
      "Loading on: 2011-1501-2000.txt\n",
      "Loading on: 1985-1-38.txt\n",
      "Loading on: 2016-2001-2500.txt\n",
      "Loading on: 2010-1001-1500.txt\n",
      "Loading on: 1996-1-357.txt\n",
      "Loading on: 2007-1001-1500.txt\n",
      "Loading on: 1990-1-93.txt\n",
      "Loading on: 1976-1-16.txt\n",
      "Loading on: 2012-2001-2500.txt\n",
      "Loading on: 1984-1-39.txt\n",
      "Loading on: 2014-1001-1500.txt\n",
      "Loading on: 2015-2001-2500.txt\n",
      "Loading on: 2004-501-1000.txt\n",
      "Loading on: 2013-1001-1500.txt\n",
      "Loading on: 2011-2001-2500.txt\n",
      "Loading on: 2005-1-500.txt\n",
      "Loading on: 2017-1001-1500.txt\n",
      "Loading on: 2011-1-500.txt\n",
      "Loading on: 2011-501-1000.txt\n",
      "Loading on: 2016-501-1000.txt\n",
      "Loading on: 2007-1501-1701.txt\n",
      "Loading on: 2016-3001-3500.txt\n",
      "Loading on: 1979-1-24.txt\n",
      "Loading on: 2013-2501-3000.txt\n",
      "Loading on: 1988-1-58.txt\n",
      "Loading on: 2016-1501-2000.txt\n",
      "Loading on: 2008-1-500.txt\n",
      "Loading on: 2016-3501-3935.txt\n",
      "Loading on: 2012-1501-2000.txt\n",
      "Loading on: 1980-1-21.txt\n",
      "Loading on: 2017-2501-3000.txt\n",
      "Loading on: 2016-1-500.txt\n",
      "Loading on: 2017-3501-4000.txt\n",
      "Loading on: 2002-1-500.txt\n",
      "Loading on: 2018-501-522.txt\n",
      "Loading on: 1986-1-36.txt\n",
      "Loading on: 1995-1-300.txt\n",
      "Loading on: 2017-4001-4142.txt\n",
      "Loading on: 1997-1-381.txt\n",
      "Loading on: 2009-501-1000.txt\n",
      "Loading on: 2009-1001-1500.txt\n",
      "Loading on: 2010-1-500.txt\n",
      "Loading on: 2003-501-844.txt\n",
      "Loading on: 2004-1-500.txt\n",
      "Loading on: 1987-1-30.txt\n",
      "Loading on: 2014-3501-3633.txt\n",
      "Loading on: 2008-501-1000.txt\n",
      "Loading on: 2004-1001-1004.txt\n",
      "Loading on: 2013-3001-3222.txt\n",
      "Loading on: 2015-3501-3748.txt\n",
      "Loading on: 2011-2501-2571.txt\n",
      "Loading on: 2009-1-500.txt\n",
      "Loading on: 2008-1501-1904.txt\n"
     ]
    }
   ],
   "source": [
    "whole_file = concat_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
