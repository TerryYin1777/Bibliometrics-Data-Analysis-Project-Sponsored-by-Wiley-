{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk import word_tokenize,ne_chunk,pos_tag,wordpunct_tokenize\n",
    "import rake_nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from itertools import chain, groupby, product\n",
    "from collections import defaultdict, Counter\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import ttest_ind\n",
    "from scipy.stats import normaltest\n",
    "% matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2,6,34,48,50,56,58,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "all_data_store_path = '/Users/tylio/Google Drive/2018 spring/686/Wiley Sponsered Project/Data'\n",
    "current_data_folder_name = '(40000+)_by_year'\n",
    "current_data_store_path = os.path.join(all_data_store_path,current_data_folder_name)\n",
    "data = pd.read_csv(os.path.join(current_data_store_path,'whole_file.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Keywords from Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain, groupby, product\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "class my_Rake(object):\n",
    "\n",
    "    def __init__(self, stopwords=None, punctuations=None, language='english'):\n",
    "        # If stopwords not provided we use language stopwords by default.\n",
    "        self.stopwords = stopwords\n",
    "        if self.stopwords is None:\n",
    "            self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "\n",
    "        # If punctuations are not provided we ignore all punctuation symbols.\n",
    "        self.punctuations = punctuations\n",
    "        if self.punctuations is None:\n",
    "            self.punctuations = string.punctuation\n",
    "\n",
    "        # All things which act as sentence breaks during keyword extraction.\n",
    "        self.to_ignore = set(chain(self.stopwords, self.punctuations))\n",
    "\n",
    "        # Stuff to be extracted from the provided text.\n",
    "        self.frequency_dist = None\n",
    "        self.degree = None\n",
    "        self.rank_list = None\n",
    "        self.ranked_phrases = None\n",
    "\n",
    "    def extract_keywords_from_text(self, text):\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "        self.extract_keywords_from_sentences(sentences)\n",
    "\n",
    "    def extract_keywords_from_sentences(self, sentences):\n",
    "        phrase_list = self._generate_phrases(sentences)\n",
    "        self._build_frequency_dist(phrase_list)\n",
    "        self._build_word_co_occurance_graph(phrase_list)\n",
    "        self._build_ranklist(phrase_list)\n",
    "\n",
    "    def get_ranked_phrases(self):\n",
    "        return self.ranked_phrases\n",
    "\n",
    "    def get_ranked_phrases_with_scores(self):\n",
    "        return self.rank_list\n",
    "\n",
    "    def get_word_frequency_distribution(self):\n",
    "        return self.frequency_dist\n",
    "\n",
    "    def get_word_degrees(self):\n",
    "        return self.degree\n",
    "\n",
    "    def _build_frequency_dist(self, phrase_list):\n",
    "        self.frequency_dist = Counter(chain.from_iterable(phrase_list))\n",
    "\n",
    "    def _build_word_co_occurance_graph(self, phrase_list):\n",
    "        co_occurance_graph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for phrase in phrase_list:\n",
    "            # For each phrase in the phrase list, count co-occurances of the\n",
    "            # word with other words in the phrase.\n",
    "            #\n",
    "            # Note: Keep the co-occurances graph as is, to help facilitate its\n",
    "            # use in other creative ways if required later.\n",
    "            for (word, coword) in product(phrase, phrase):\n",
    "                co_occurance_graph[word][coword] += 1\n",
    "        self.degree = defaultdict(lambda: 0)\n",
    "        for key in co_occurance_graph:\n",
    "            self.degree[key] = sum(co_occurance_graph[key].values())\n",
    "\n",
    "    def _build_ranklist(self, phrase_list):\n",
    "        self.rank_list = []\n",
    "        for phrase in phrase_list:\n",
    "            rank = 0.0\n",
    "            for word in phrase:\n",
    "                rank += 1.0 * self.degree[word] / self.frequency_dist[word]\n",
    "            self.rank_list.append((rank, ' '.join(phrase)))\n",
    "        self.rank_list.sort(reverse=True)\n",
    "        self.ranked_phrases = [ph[1] for ph in self.rank_list]\n",
    "\n",
    "    def _generate_phrases(self, sentences):\n",
    "        phrase_list = set()\n",
    "        # Create contender phrases from sentences.\n",
    "        lem = WordNetLemmatizer()\n",
    "        for sentence in sentences:\n",
    "            word_list = [lem.lemmatize(word.lower()) for word in word_tokenize(sentence)]\n",
    "            phrase_list.update(self._get_phrase_list_from_words(word_list))\n",
    "        return phrase_list\n",
    "\n",
    "    def _get_phrase_list_from_words(self, word_list):\n",
    "        groups = groupby(word_list, lambda x: x not in self.to_ignore and pos_tag([x])[0][1]!='VBG')\n",
    "        return [tuple(group[1]) for group in groups if group[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keyword_extractor(data):\n",
    "    extractor = my_Rake()\n",
    "    extractor.extract_keywords_from_text(data['TI'])\n",
    "    ti = ';'.join([i[1] for i in extractor.get_ranked_phrases_with_scores()])\n",
    "    if type(data['DE'])==float:\n",
    "        return ti\n",
    "    else:\n",
    "        extractor.extract_keywords_from_text(data['DE'])\n",
    "        de = ';'.join([i[1] for i in extractor.get_ranked_phrases_with_scores()])\n",
    "        return ti+';'+de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_words = data.apply(lambda row: keyword_extractor(row),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,'key_words'] = key_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out Important Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw_filter = set(chain(string.punctuation,stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw_candidate = ';'.join(list(data['key_words'])).split(';')\n",
    "kw_pds_original = pd.Series(kw_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = list(kw_pds_original.value_counts()[kw_pds_original.value_counts()>100].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we got 230 candidate keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the Document Index for Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key_words_candidate = data['key_words'].apply(lambda x: ' '+(' ; ').join(x.split(';'))+' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,'key_words_candidate'] = key_words_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_docIndex = {}\n",
    "kwc = data['key_words_candidate']\n",
    "for topic in topics:\n",
    "    topic_docIndex[topic] = kwc[kwc.apply(lambda x: True if ' '+topic+' ' in x else False)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_map = pd.DataFrame(index = topic_docIndex.keys(),columns=['doc_index'])\n",
    "for topic in topic_docIndex.keys():\n",
    "    topic_map.loc[topic,'doc_index'] = topic_docIndex[topic]\n",
    "topic_map = topic_map.reset_index().rename(columns = {'index':'topic'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>doc_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>polyester</td>\n",
       "      <td>[105, 125, 274, 326, 392, 549, 560, 582, 652, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>investigation</td>\n",
       "      <td>[302, 432, 656, 659, 777, 802, 827, 1038, 1148...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>biofilm</td>\n",
       "      <td>[147, 159, 231, 272, 611, 678, 712, 837, 896, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>self-assembly</td>\n",
       "      <td>[35, 163, 292, 460, 480, 523, 541, 545, 594, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>medical device</td>\n",
       "      <td>[23, 233, 256, 957, 1404, 2003, 2100, 2239, 22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            topic                                          doc_index\n",
       "0       polyester  [105, 125, 274, 326, 392, 549, 560, 582, 652, ...\n",
       "1   investigation  [302, 432, 656, 659, 777, 802, 827, 1038, 1148...\n",
       "2         biofilm  [147, 159, 231, 272, 611, 678, 712, 837, 896, ...\n",
       "3   self-assembly  [35, 163, 292, 460, 480, 523, 541, 545, 594, 5...\n",
       "4  medical device  [23, 233, 256, 957, 1404, 2003, 2100, 2239, 22..."
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_map.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate the Topics with Similar Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_doc_map_1gram = topic_map[topic_map['topic'].str.split(' ').apply(len)==1]\n",
    "topic_doc_map_2gram = topic_map[topic_map['topic'].str.split(' ').apply(len)==2]\n",
    "topic_doc_map_3gram = topic_map[topic_map['topic'].str.split(' ').apply(len)==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similar_score(s1,s2):\n",
    "    union1 = len(set(s1))\n",
    "    union2 = len(set(s2))\n",
    "    union = len(set(np.concatenate((s1,s2))))\n",
    "    intersection = len(s1)+len(s2)-union\n",
    "    score1 = intersection/union1\n",
    "    score2 = intersection/union2\n",
    "    return score1,score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def duplication_detection(df1,df2):\n",
    "    delete_dict = {}\n",
    "    for index_1 in df1.index:\n",
    "        for index_2 in df2.index:\n",
    "            if df1['topic'][index_1] in df2['topic'][index_2]:\n",
    "                s1 = df1['doc_index'][index_1]\n",
    "                s2 = df2['doc_index'][index_2]\n",
    "                score1,score2 = similar_score(s1,s2)\n",
    "                threshold = 0.9\n",
    "                if score1>threshold and score2>threshold:\n",
    "                    if score1>=score2:\n",
    "                        delete_dict[df1['topic'][index_1]] = True\n",
    "                    else:\n",
    "                        delete_dict[df2['topic'][index_2]] = True\n",
    "#                 if score1>threshold:\n",
    "#                     delete_dict[df1['topic'][index_1]] = True\n",
    "#                 if score2>threshold:\n",
    "#                     delete_dict[df2['topic'][index_2]] = True\n",
    "    return delete_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_duplication(original_df,delete_dict):\n",
    "    transformed_df = original_df.copy()\n",
    "    transformed_df = transformed_df[~transformed_df['topic'].isin(delete_dict.keys())]\n",
    "    return transformed_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def merge_duplications(original_df,similar_dict):\n",
    "    transformed_df = original_df.copy()\n",
    "    for old_topic in similar_dict:\n",
    "        new_topic = similar_dict[old_topic]\n",
    "        s1 = transformed_df.loc[transformed_df['topic']==old_topic,'doc_index'].values[0]\n",
    "        s2 = transformed_df.loc[transformed_df['topic']==new_topic,'doc_index'].values[0]\n",
    "        new_s = np.array(list(set(np.concatenate((s1,s2)))))\n",
    "        index = transformed_df[transformed_df['topic']==new_topic].index.values[0]\n",
    "        transformed_df.loc[index,'doc_index'] = new_s\n",
    "    transformed_df = transformed_df[~transformed_df['topic'].isin(similar_dict.keys())]\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_map = delete_duplication(topic_map,duplication_detection(topic_doc_map_2gram,topic_doc_map_3gram))\n",
    "topic_map = delete_duplication(topic_map,duplication_detection(topic_doc_map_1gram,topic_doc_map_2gram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#topic_map.to_csv('topic_map.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in topic_map.index:\n",
    "    data.loc[topic_map['doc_index'][i],topic_map['topic'][i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.to_csv('wholeFile_with_wholeTopic_v3.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features for Topic Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_data_store_path = '/Users/tylio/Google Drive/2018 spring/686/Wiley Sponsered Project/Data'\n",
    "# current_data_folder_name = '(40000+)_by_year'\n",
    "# current_data_store_path = os.path.join(all_data_store_path,current_data_folder_name)\n",
    "# data = pd.read_csv(os.path.join(current_data_store_path,'wholeFile_with_wholeTopic.csv'))\n",
    "data = data[(data['PY']>=1995) & (data['PY']<2018)]\n",
    "# topics = pd.read_csv(os.path.join(current_data_store_path,'topic_map.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_percentage_yealy(df,topic):\n",
    "    percentage = pd.concat([df.groupby('PY')[topic].count(),df.groupby('PY')['DA'].count()],axis = 1)\n",
    "    percentage.loc[:,'percentage'] = percentage.apply(lambda x: x[topic]/x['DA'],axis = 1)\n",
    "    return percentage['percentage']\n",
    "\n",
    "def topics_trends_main(df,topic_map):\n",
    "    i = 0\n",
    "    for topic in topic_map['topic']:\n",
    "        topic_percentage = topic_percentage_yealy(df,topic)\n",
    "        for year in topic_percentage.index:\n",
    "            topic_map.loc[topic_map['topic']==topic,str(int(year))] = topic_percentage[year]\n",
    "        i+=1\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "topics_trends_main(data,topic_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1995,2018):\n",
    "    topic_map[str(i)+'(acc)'] = topic_map[str(i)]-topic_map['1995']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in topic_map.index:\n",
    "    for j in range(1996,2018):\n",
    "        sample = topic_map.loc[i,str(j)]-topic_map.loc[i,str(j-1)]\n",
    "        topic_map.loc[i,str(j)+'(gap)'] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_map['doc_index'] = topic_map['doc_index'].apply(lambda x: ';'.join([str(i) for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_map.to_csv(os.path.join(current_data_store_path,'topic_map_v2_2.csv'),index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(os.path.join(current_data_store_path,'wholeFile_with_wholeTopic_v2_2.csv'),index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
